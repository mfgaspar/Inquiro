<?xml version="1.0" encoding="UTF-8"?>
<transformation>
  <info>
    <name>capture_hadoop_metrics</name>
    <description/>
    <extended_description/>
    <trans_version/>
    <trans_type>Normal</trans_type>
    <trans_status>0</trans_status>
    <directory>&#x2f;</directory>
    <parameters>
    </parameters>
    <log>
<trans-log-table><connection/>
<schema/>
<table/>
<size_limit_lines/>
<interval/>
<timeout_days/>
<field><id>ID_BATCH</id><enabled>Y</enabled><name>ID_BATCH</name></field><field><id>CHANNEL_ID</id><enabled>Y</enabled><name>CHANNEL_ID</name></field><field><id>TRANSNAME</id><enabled>Y</enabled><name>TRANSNAME</name></field><field><id>STATUS</id><enabled>Y</enabled><name>STATUS</name></field><field><id>LINES_READ</id><enabled>Y</enabled><name>LINES_READ</name><subject/></field><field><id>LINES_WRITTEN</id><enabled>Y</enabled><name>LINES_WRITTEN</name><subject/></field><field><id>LINES_UPDATED</id><enabled>Y</enabled><name>LINES_UPDATED</name><subject/></field><field><id>LINES_INPUT</id><enabled>Y</enabled><name>LINES_INPUT</name><subject/></field><field><id>LINES_OUTPUT</id><enabled>Y</enabled><name>LINES_OUTPUT</name><subject/></field><field><id>LINES_REJECTED</id><enabled>Y</enabled><name>LINES_REJECTED</name><subject/></field><field><id>ERRORS</id><enabled>Y</enabled><name>ERRORS</name></field><field><id>STARTDATE</id><enabled>Y</enabled><name>STARTDATE</name></field><field><id>ENDDATE</id><enabled>Y</enabled><name>ENDDATE</name></field><field><id>LOGDATE</id><enabled>Y</enabled><name>LOGDATE</name></field><field><id>DEPDATE</id><enabled>Y</enabled><name>DEPDATE</name></field><field><id>REPLAYDATE</id><enabled>Y</enabled><name>REPLAYDATE</name></field><field><id>LOG_FIELD</id><enabled>Y</enabled><name>LOG_FIELD</name></field><field><id>EXECUTING_SERVER</id><enabled>N</enabled><name>EXECUTING_SERVER</name></field><field><id>EXECUTING_USER</id><enabled>N</enabled><name>EXECUTING_USER</name></field><field><id>CLIENT</id><enabled>N</enabled><name>CLIENT</name></field></trans-log-table>
<perf-log-table><connection/>
<schema/>
<table/>
<interval/>
<timeout_days/>
<field><id>ID_BATCH</id><enabled>Y</enabled><name>ID_BATCH</name></field><field><id>SEQ_NR</id><enabled>Y</enabled><name>SEQ_NR</name></field><field><id>LOGDATE</id><enabled>Y</enabled><name>LOGDATE</name></field><field><id>TRANSNAME</id><enabled>Y</enabled><name>TRANSNAME</name></field><field><id>STEPNAME</id><enabled>Y</enabled><name>STEPNAME</name></field><field><id>STEP_COPY</id><enabled>Y</enabled><name>STEP_COPY</name></field><field><id>LINES_READ</id><enabled>Y</enabled><name>LINES_READ</name></field><field><id>LINES_WRITTEN</id><enabled>Y</enabled><name>LINES_WRITTEN</name></field><field><id>LINES_UPDATED</id><enabled>Y</enabled><name>LINES_UPDATED</name></field><field><id>LINES_INPUT</id><enabled>Y</enabled><name>LINES_INPUT</name></field><field><id>LINES_OUTPUT</id><enabled>Y</enabled><name>LINES_OUTPUT</name></field><field><id>LINES_REJECTED</id><enabled>Y</enabled><name>LINES_REJECTED</name></field><field><id>ERRORS</id><enabled>Y</enabled><name>ERRORS</name></field><field><id>INPUT_BUFFER_ROWS</id><enabled>Y</enabled><name>INPUT_BUFFER_ROWS</name></field><field><id>OUTPUT_BUFFER_ROWS</id><enabled>Y</enabled><name>OUTPUT_BUFFER_ROWS</name></field></perf-log-table>
<channel-log-table><connection/>
<schema/>
<table/>
<timeout_days/>
<field><id>ID_BATCH</id><enabled>Y</enabled><name>ID_BATCH</name></field><field><id>CHANNEL_ID</id><enabled>Y</enabled><name>CHANNEL_ID</name></field><field><id>LOG_DATE</id><enabled>Y</enabled><name>LOG_DATE</name></field><field><id>LOGGING_OBJECT_TYPE</id><enabled>Y</enabled><name>LOGGING_OBJECT_TYPE</name></field><field><id>OBJECT_NAME</id><enabled>Y</enabled><name>OBJECT_NAME</name></field><field><id>OBJECT_COPY</id><enabled>Y</enabled><name>OBJECT_COPY</name></field><field><id>REPOSITORY_DIRECTORY</id><enabled>Y</enabled><name>REPOSITORY_DIRECTORY</name></field><field><id>FILENAME</id><enabled>Y</enabled><name>FILENAME</name></field><field><id>OBJECT_ID</id><enabled>Y</enabled><name>OBJECT_ID</name></field><field><id>OBJECT_REVISION</id><enabled>Y</enabled><name>OBJECT_REVISION</name></field><field><id>PARENT_CHANNEL_ID</id><enabled>Y</enabled><name>PARENT_CHANNEL_ID</name></field><field><id>ROOT_CHANNEL_ID</id><enabled>Y</enabled><name>ROOT_CHANNEL_ID</name></field></channel-log-table>
<step-log-table><connection/>
<schema/>
<table/>
<timeout_days/>
<field><id>ID_BATCH</id><enabled>Y</enabled><name>ID_BATCH</name></field><field><id>CHANNEL_ID</id><enabled>Y</enabled><name>CHANNEL_ID</name></field><field><id>LOG_DATE</id><enabled>Y</enabled><name>LOG_DATE</name></field><field><id>TRANSNAME</id><enabled>Y</enabled><name>TRANSNAME</name></field><field><id>STEPNAME</id><enabled>Y</enabled><name>STEPNAME</name></field><field><id>STEP_COPY</id><enabled>Y</enabled><name>STEP_COPY</name></field><field><id>LINES_READ</id><enabled>Y</enabled><name>LINES_READ</name></field><field><id>LINES_WRITTEN</id><enabled>Y</enabled><name>LINES_WRITTEN</name></field><field><id>LINES_UPDATED</id><enabled>Y</enabled><name>LINES_UPDATED</name></field><field><id>LINES_INPUT</id><enabled>Y</enabled><name>LINES_INPUT</name></field><field><id>LINES_OUTPUT</id><enabled>Y</enabled><name>LINES_OUTPUT</name></field><field><id>LINES_REJECTED</id><enabled>Y</enabled><name>LINES_REJECTED</name></field><field><id>ERRORS</id><enabled>Y</enabled><name>ERRORS</name></field><field><id>LOG_FIELD</id><enabled>N</enabled><name>LOG_FIELD</name></field></step-log-table>
<metrics-log-table><connection/>
<schema/>
<table/>
<timeout_days/>
<field><id>ID_BATCH</id><enabled>Y</enabled><name>ID_BATCH</name></field><field><id>CHANNEL_ID</id><enabled>Y</enabled><name>CHANNEL_ID</name></field><field><id>LOG_DATE</id><enabled>Y</enabled><name>LOG_DATE</name></field><field><id>METRICS_DATE</id><enabled>Y</enabled><name>METRICS_DATE</name></field><field><id>METRICS_CODE</id><enabled>Y</enabled><name>METRICS_CODE</name></field><field><id>METRICS_DESCRIPTION</id><enabled>Y</enabled><name>METRICS_DESCRIPTION</name></field><field><id>METRICS_SUBJECT</id><enabled>Y</enabled><name>METRICS_SUBJECT</name></field><field><id>METRICS_TYPE</id><enabled>Y</enabled><name>METRICS_TYPE</name></field><field><id>METRICS_VALUE</id><enabled>Y</enabled><name>METRICS_VALUE</name></field></metrics-log-table>
    </log>
    <maxdate>
      <connection/>
      <table/>
      <field/>
      <offset>0.0</offset>
      <maxdiff>0.0</maxdiff>
    </maxdate>
    <size_rowset>10000</size_rowset>
    <sleep_time_empty>50</sleep_time_empty>
    <sleep_time_full>50</sleep_time_full>
    <unique_connections>N</unique_connections>
    <feedback_shown>Y</feedback_shown>
    <feedback_size>50000</feedback_size>
    <using_thread_priorities>Y</using_thread_priorities>
    <shared_objects_file/>
    <capture_step_performance>N</capture_step_performance>
    <step_performance_capturing_delay>1000</step_performance_capturing_delay>
    <step_performance_capturing_size_limit>100</step_performance_capturing_size_limit>
    <dependencies>
    </dependencies>
    <partitionschemas>
    </partitionschemas>
    <slaveservers>
    </slaveservers>
    <clusterschemas>
    </clusterschemas>
  <created_user>-</created_user>
  <created_date>2013&#x2f;10&#x2f;10 08&#x3a;24&#x3a;47.332</created_date>
  <modified_user>-</modified_user>
  <modified_date>2013&#x2f;10&#x2f;10 08&#x3a;24&#x3a;47.332</modified_date>
  </info>
  <notepads>
  </notepads>
  <connection>
    <name>AgileBI</name>
    <server>localhost</server>
    <type>MONETDB</type>
    <access>Native</access>
    <database>pentaho-instaview</database>
    <port>50006</port>
    <username>monetdb</username>
    <password>Encrypted 2be98afc86aa7f2e4cb14a17edb86abd8</password>
    <servername/>
    <data_tablespace/>
    <index_tablespace/>
    <read_only>true</read_only>
    <attributes>
      <attribute><code>EXTRA_OPTION_INFOBRIGHT.characterEncoding</code><attribute>UTF-8</attribute></attribute>
      <attribute><code>EXTRA_OPTION_MYSQL.defaultFetchSize</code><attribute>500</attribute></attribute>
      <attribute><code>EXTRA_OPTION_MYSQL.useCursorFetch</code><attribute>true</attribute></attribute>
      <attribute><code>PORT_NUMBER</code><attribute>50006</attribute></attribute>
      <attribute><code>SUPPORTS_BOOLEAN_DATA_TYPE</code><attribute>Y</attribute></attribute>
      <attribute><code>SUPPORTS_TIMESTAMP_DATA_TYPE</code><attribute>Y</attribute></attribute>
    </attributes>
  </connection>
  <order>
  <hop> <from>UDJC&#x3a; Retrieve Hadoop Counters</from><to>Write to log</to><enabled>N</enabled> </hop>
  <hop> <from>UDJC&#x3a; Retrieve Hadoop Counters</from><to>Text file output</to><enabled>N</enabled> </hop>
  <hop> <from>Generate Rows</from><to>Get Variables</to><enabled>Y</enabled> </hop>
  <hop> <from>Get Variables</from><to>UDJC&#x3a; Retrieve Hadoop Counters</to><enabled>Y</enabled> </hop>
  <hop> <from>UDJC&#x3a; Retrieve Hadoop Counters</from><to>Modified Java Script Value</to><enabled>Y</enabled> </hop>
  <hop> <from>Modified Java Script Value</from><to>Write Metrics to LogFile on HDFS</to><enabled>Y</enabled> </hop>
  </order>
  <step>
    <name>Generate Rows</name>
    <type>RowGenerator</type>
    <description/>
    <distribute>Y</distribute>
    <custom_distribution/>
    <copies>1</copies>
         <partitioning>
           <method>none</method>
           <schema_name/>
           </partitioning>
    <fields>
      <field>
        <name>dummy</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif>junk</nullif>
        <length>-1</length>
        <precision>-1</precision>
        <set_empty_string>N</set_empty_string>
      </field>
    </fields>
    <limit>1</limit>
    <never_ending>N</never_ending>
    <interval_in_ms>5000</interval_in_ms>
    <row_time_field>now</row_time_field>
    <last_time_field>FiveSecondsAgo</last_time_field>
     <cluster_schema/>
 <remotesteps>   <input>   </input>   <output>   </output> </remotesteps>    <GUI>
      <xloc>42</xloc>
      <yloc>190</yloc>
      <draw>Y</draw>
      </GUI>
    </step>

  <step>
    <name>Get Variables</name>
    <type>GetVariable</type>
    <description/>
    <distribute>Y</distribute>
    <custom_distribution/>
    <copies>1</copies>
         <partitioning>
           <method>none</method>
           <schema_name/>
           </partitioning>
    <fields>
      <field>
        <name>hdfsFolder</name>
        <variable>&#x24;&#x7b;hadoopMetricsFolder&#x7d;</variable>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <length>-1</length>
        <precision>-1</precision>
        <trim_type>none</trim_type>
      </field>
      <field>
        <name>hdfsHost</name>
        <variable>&#x24;&#x7b;hadoop_hdfs_host&#x7d;</variable>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <length>-1</length>
        <precision>-1</precision>
        <trim_type>none</trim_type>
      </field>
      <field>
        <name>hdfsPort</name>
        <variable>&#x24;&#x7b;hadoop_hdfs_port&#x7d;</variable>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <length>-1</length>
        <precision>-1</precision>
        <trim_type>none</trim_type>
      </field>
      <field>
        <name>hadoopLogFilePath</name>
        <variable>&#x24;&#x7b;hadoop_log_file_path&#x7d;</variable>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <length>-1</length>
        <precision>-1</precision>
        <trim_type>none</trim_type>
      </field>
      <field>
        <name>hdfsMetricsOutputPath</name>
        <variable>&#x24;&#x7b;hdfs_metrics_output_path&#x7d;</variable>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <length>-1</length>
        <precision>-1</precision>
        <trim_type>none</trim_type>
      </field>
    </fields>
     <cluster_schema/>
 <remotesteps>   <input>   </input>   <output>   </output> </remotesteps>    <GUI>
      <xloc>168</xloc>
      <yloc>191</yloc>
      <draw>Y</draw>
      </GUI>
    </step>

  <step>
    <name>Modified Java Script Value</name>
    <type>ScriptValueMod</type>
    <description/>
    <distribute>Y</distribute>
    <custom_distribution/>
    <copies>1</copies>
         <partitioning>
           <method>none</method>
           <schema_name/>
           </partitioning>
    <compatible>N</compatible>
    <optimizationLevel>9</optimizationLevel>
    <jsScripts>      <jsScript>        <jsScript_type>0</jsScript_type>
        <jsScript_name>Script 1</jsScript_name>
        <jsScript_script>&#x2f;&#x2f;Script here&#xa;&#xa;var hdfsPath &#x3d; hdfsMetricsOutputPath&#x2b;&#x27;&#x2f;&#x27;&#x2b;hadoop_job_id&#xa;var hadoopLogPath &#x3d; hadoopLogFilePath.replace&#x28;&#x27;JOB&#x27;, hadoop_job_id&#x29;&#x3b;</jsScript_script>
      </jsScript>    </jsScripts>    <fields>      <field>        <name>hdfsPath</name>
        <rename>hdfsPath</rename>
        <type>String</type>
        <length>-1</length>
        <precision>-1</precision>
        <replace>N</replace>
      </field>      <field>        <name>hadoopLogPath</name>
        <rename>hadoopLogPath</rename>
        <type>String</type>
        <length>-1</length>
        <precision>-1</precision>
        <replace>N</replace>
      </field>    </fields>     <cluster_schema/>
 <remotesteps>   <input>   </input>   <output>   </output> </remotesteps>    <GUI>
      <xloc>556</xloc>
      <yloc>107</yloc>
      <draw>Y</draw>
      </GUI>
    </step>

  <step>
    <name>Text file output</name>
    <type>TextFileOutput</type>
    <description/>
    <distribute>Y</distribute>
    <custom_distribution/>
    <copies>1</copies>
         <partitioning>
           <method>none</method>
           <schema_name/>
           </partitioning>
    <separator>&#x3b;</separator>
    <enclosure>&#x22;</enclosure>
    <enclosure_forced>N</enclosure_forced>
    <enclosure_fix_disabled>N</enclosure_fix_disabled>
    <header>Y</header>
    <footer>N</footer>
    <format>UNIX</format>
    <compression>None</compression>
    <encoding>UTF-8</encoding>
    <endedLine/>
    <fileNameInField>N</fileNameInField>
    <fileNameField/>
    <create_parent_folder>N</create_parent_folder>
    <file>
      <name>&#x2f;home&#x2f;zeus&#x2f;Desktop&#x2f;get_hadoop_metrics&#x2f;output</name>
      <is_command>N</is_command>
      <servlet_output>N</servlet_output>
      <do_not_open_new_file_init>N</do_not_open_new_file_init>
      <extention>txt</extention>
      <append>N</append>
      <split>N</split>
      <haspartno>N</haspartno>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <SpecifyFormat>N</SpecifyFormat>
      <date_time_format/>
      <add_to_result_filenames>N</add_to_result_filenames>
      <pad>N</pad>
      <fast_dump>N</fast_dump>
      <splitevery>0</splitevery>
    </file>
    <fields>
      <field>
        <name>dummy</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>counter_type</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>hadoop_job_id</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>counter_name</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>start_time</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>counter_group</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>counter_desc</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>counter_val</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>hadoop_task_id</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>general_info</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
    </fields>
     <cluster_schema/>
 <remotesteps>   <input>   </input>   <output>   </output> </remotesteps>    <GUI>
      <xloc>556</xloc>
      <yloc>266</yloc>
      <draw>Y</draw>
      </GUI>
    </step>

  <step>
    <name>UDJC&#x3a; Retrieve Hadoop Counters</name>
    <type>UserDefinedJavaClass</type>
    <description/>
    <distribute>N</distribute>
    <custom_distribution/>
    <copies>1</copies>
         <partitioning>
           <method>none</method>
           <schema_name/>
           </partitioning>

    <definitions>
        <definition>
        <class_type>TRANSFORM_CLASS</class_type>

        <class_name>Processor</class_name>

        <class_source><![CDATA[
import java.net.InetSocketAddress;
import java.util.TreeMap;
import java.util.Iterator;
import java.util.Map;
import java.util.Set;


import org.apache.hadoop.conf.Configuration;

import org.apache.hadoop.mapred.Counters;
import org.apache.hadoop.mapred.JobClient;
import org.apache.hadoop.mapred.JobID;
import org.apache.hadoop.mapred.JobPriority;
import org.apache.hadoop.mapred.JobStatus;
import org.apache.hadoop.mapred.RunningJob;
import org.apache.hadoop.mapred.TaskReport;



protected static String strKeyPrefixInfo = "Info";
protected static String strKeyPrefixCounter = "Counter";

public boolean processRow(StepMetaInterface smi, StepDataInterface sdi) throws KettleException
{
    Object[] r = getRow();
    if (r == null) {
        setOutputDone();
        return false;
    }

    if (first)
    {
        first = false;
    }

    Map mapResult = getHadoopCounters(  getVariable("hadoop_jobtracker_host", ""), 
                                        Integer.parseInt(getVariable("hadoop_jobtracker_port", "")),
                                        getVariable("hadoop_job_name", ""), true);
    

    // Iterate over the groups
    Set sGroup = mapResult.entrySet();
    Iterator itGroup = sGroup.iterator();

    String[] arStrA;
    String[] arStrB;
    String strCounterType;
    String strHadoopJobId;
    String strCounterPrefix = strKeyPrefixCounter + ":";

    while(itGroup.hasNext())
    {
        // key=value separator this by Map.Entry to get key and value
        Map.Entry entryGroup =(Map.Entry)itGroup.next();

        // getKey is used to get key of Map
        String strType=(String)entryGroup.getKey();

        // getValue is used to get value of key in Map
        Map mapData=(Map)entryGroup.getValue();

        // Set Common Fileds for this goup of counters
        arStrA = strType.split(":");
        strCounterType = arStrA[0];
        strHadoopJobId = (String) mapData.get(strKeyPrefixInfo + ":JobId");


        // Iterate over the key/values in group
        Set sData = mapData.entrySet();
        Iterator itData = sData.iterator();

        while(itData.hasNext())
        {
            Map.Entry entryData =(Map.Entry)itData.next();

            String strKey=(String)entryData.getKey();
            String strVal = (String)entryData.getValue();
            logBasic("\t" + strKey + "=" + strVal);

            Object[] outputRow = createOutputRow(r, data.outputRowMeta.size());
            if (strKey.startsWith(strCounterPrefix))
            {
                // Create output row

                get(Fields.Out, "counter_type").setValue(outputRow, strCounterType);
                get(Fields.Out, "hadoop_job_id").setValue(outputRow, strHadoopJobId);

                arStrA = strVal.split("\\|");

                arStrB = arStrA[0].split("=");
                get(Fields.Out, "counter_group").setValue(outputRow, arStrB[1]);

                arStrB = arStrA[1].split("=");
                get(Fields.Out, "counter_name").setValue(outputRow, arStrB[1]);

                arStrB = arStrA[2].split("=");
                get(Fields.Out, "counter_desc").setValue(outputRow, arStrB[1]);

                arStrB = arStrA[3].split("=");
                get(Fields.Out, "counter_val").setValue(outputRow, arStrB[1]);

                arStrB = arStrA[4].split("=");
                get(Fields.Out, "hadoop_task_id").setValue(outputRow, arStrB[1]);

                arStrB = arStrA[5].split("=");
                get(Fields.Out, "start_time").setValue(outputRow, arStrB[1]);

                putRow(data.outputRowMeta, outputRow);
            }
            else
            {
                get(Fields.Out, "counter_type").setValue(outputRow, strCounterType);
                get(Fields.Out, "hadoop_job_id").setValue(outputRow, strHadoopJobId);
                get(Fields.Out, "general_info").setValue(outputRow, strKey+"="+strVal);

                putRow(data.outputRowMeta, outputRow);
            }

        }
    }

    return true;
}


/**
 * @param strJobTrackHost
 * @param strJobTrackPort
 * @param strWantedJobName
 * @param bCompletedJobsOnly
 * @return - a map of all the values
 *          Map<Group, Map <CounterName, CounterValue>
 */
public Map getHadoopCounters (String strJobTrackHost, int strJobTrackPort, String strWantedJobName, boolean bCompletedJobsOnly)
{
    Map mapResult=new TreeMap();
    int iTasksCount = 0;
    int iJobCount = 0;

    try
    {
        JobClient jc = new JobClient(new InetSocketAddress(strJobTrackHost, strJobTrackPort), new Configuration());
        JobStatus[] jobStatuses = jc.getAllJobs();
        
        for (int i = 0; i < jobStatuses.length; i++)
        {
            JobStatus jstat = jobStatuses[i];
                     
            // Check if the we want completed jobs only
            if (bCompletedJobsOnly ? jstat.isJobComplete() : true)
            {
                JobID jid = jstat.getJobID();
                RunningJob jobinfo = jc.getJob(jid);
                
                // Check if we want a specific job
                if (strWantedJobName == null ? true : jobinfo.getJobName().equals(strWantedJobName) )
                {
                    Map mapJobInfo= getHadoopJobInfo(jobinfo);
                    Object[] arObj = {iJobCount++};
                    mapResult.put(String.format("JOB_INFO:%04d", arObj), mapJobInfo);
                    
                    iTasksCount += getHadoopTaskInfo (jc, jid, iTasksCount, mapResult, jstat.getUsername());
                    
                }                   
            }
        }

    }
    catch(Exception e)
    {
        logError("FAIL: ", e);
    }
    
    return mapResult;
}

/**
 * 
 * @param jobinfo
 * @return Key/Val map of counter
 */
public Map getHadoopJobInfo (RunningJob runinfo)
{
    Map mapJobInfo = new TreeMap();
    
    try
    {
        JobID jid = runinfo.getID();
        JobStatus jstat = runinfo.getJobStatus();
        Counters jobCounters = runinfo.getCounters();

        JobPriority jpriority = jstat.getJobPriority();
        int iState = jstat.getRunState();
        
        mapJobInfo.put(strKeyPrefixInfo + ":JobId", jid.toString());
        mapJobInfo.put(strKeyPrefixInfo + ":JobName", runinfo.getJobName());
        mapJobInfo.put(strKeyPrefixInfo + ":JobStartTime", String.valueOf(jstat.getStartTime()));
        mapJobInfo.put(strKeyPrefixInfo + ":JobState", String.valueOf(iState));
        mapJobInfo.put(strKeyPrefixInfo + ":JobStateDesc", JobStatus.getJobRunState(iState));
        mapJobInfo.put(strKeyPrefixInfo + ":JobPriority", String.valueOf(jpriority.ordinal()));
        mapJobInfo.put(strKeyPrefixInfo + ":JobPriorityDesc", jpriority.toString());

        mapJobInfo.put(strKeyPrefixInfo + ":JobUsername", jstat.getUsername());
        mapJobInfo.put(strKeyPrefixInfo + ":JobMapProgress", String.valueOf(jstat.mapProgress()));
        mapJobInfo.put(strKeyPrefixInfo + ":JobReduceProgress", String.valueOf(jstat.reduceProgress()));

        getHadoopCounterInfo(jobCounters, mapJobInfo, jid.toString(), String.valueOf(jstat.getStartTime()));
    }
    catch(Exception e)
    {
        logError("FAIL: ", e);
    }

    return mapJobInfo;
}


public int getHadoopTaskInfo (JobClient jc, JobID jid, int iBaseTaskIndex, Map mapOutResult, String userName)
{
    int i=0;
    
    try 
    {
        TaskReport[] taskreps;

        // Map Task Counters
        taskreps = jc.getMapTaskReports(jid);
        i += getHadoopTaskInfo(jid, taskreps, i, "MAP", mapOutResult, userName);

        // Reduce Task Counters
        taskreps = jc.getReduceTaskReports(jid);
        i += getHadoopTaskInfo(jid, taskreps, i, "REDUCE", mapOutResult, userName);
    }
    catch(Exception e)
    {
        logError("FAIL: ", e);
    }
    
    return i;
}


/**
 * 
 * @param jc
 * @param jid
 * @param mapOutResult - Will add all groups with their counters to this map
 */
public int getHadoopTaskInfo (JobID jid, TaskReport[] taskreps, int iBaseTaskIndex, String strTaskPrefix, Map mapOutResult, String userName)
{
    Object[] arObj = new Object[2];

    try
    {
        for (int itrep = 0; itrep < taskreps.length; itrep++) 
        {
            Map mapGroupCounters = new TreeMap();
            TaskReport taskrep = taskreps[itrep];

            mapGroupCounters.put(strKeyPrefixInfo + ":JobId", jid.toString());
            mapGroupCounters.put(strKeyPrefixInfo + ":TaskId", taskrep.getTaskID().toString());
            mapGroupCounters.put(strKeyPrefixInfo + ":TaskStartTime", String.valueOf(taskrep.getStartTime()));
            mapGroupCounters.put(strKeyPrefixInfo + ":TaskFinishTime", String.valueOf(taskrep.getFinishTime()));

            mapGroupCounters.put(strKeyPrefixInfo + ":TaskCurrentStatus", String.valueOf(taskrep.getCurrentStatus()));
            mapGroupCounters.put(strKeyPrefixInfo + ":TaskProgress", String.valueOf(taskrep.getProgress()));
            mapGroupCounters.put(strKeyPrefixInfo + ":TaskState", String.valueOf(taskrep.getState()));

     		//mapGroupCounters.put(strKeyPrefixInfo + ":TaskLocalDir", taskrep.getLocalTaskDir(userName, jid.toString(), taskrep.getTaskID().toString()));

            getHadoopCounterInfo(taskrep.getCounters(), mapGroupCounters, taskrep.getTaskID().toString(), String.valueOf(taskrep.getStartTime()));

            arObj[0] = strTaskPrefix;
            arObj[1] = new Integer(iBaseTaskIndex + itrep);
            mapOutResult.put(String.format("%s_TASK:%06d", arObj), mapGroupCounters);
        }

        return taskreps.length;
    }
    catch(Exception e)
    {
        logError("FAIL: ", e);
    }

    return 0;
}


/**
 * 
 * @param counters
 * @param steKeyPrefix
 * @param outMapCounters 
 *      Will output three rows for each counter where the map key will be:
 *      key=<strKeyPrefix>:<seqnum>:<CounterName>:<FieldName>
 *      Where FieldNames will be:
 *              Name - Counter Name
 *              Desc - Counter Display Name
 *              Val - Counter Value
 */
public void getHadoopCounterInfo (Counters counters, Map outMapCounters, String id, String startTime)
{
    int seqnum = 0;
    String strBaseKey = strKeyPrefixCounter + ":";

    for (Iterator iter = counters.iterator(); iter.hasNext();)
    {
        Counters.Group group = (Counters.Group) iter.next();
        String strGrpName = group.getName();
        String strGrpDesc = group.getDisplayName();
        Object[] arObj = new Object[6];

        for (Iterator itCounter = group.iterator(); itCounter.hasNext();) 
        {
            Counters.Counter counter = (Counters.Counter) itCounter.next();

            arObj[0] = new Integer(seqnum++);
            String strCounterKey = strBaseKey + String.format("%03d", arObj);
        
            arObj[0] = strGrpName;
            arObj[1] = counter.getName();
            arObj[2] = counter.getDisplayName();
            arObj[3] = String.valueOf(counter.getValue());
            arObj[4] = id;
            arObj[5] = startTime;

            //String strCounterVal = String.format("group=%s|name=%s|desc=%s|val=%s|id=%s", arObj);
            String strCounterVal = String.format("group=%s|name=%s|desc=%s|val=%s|id=%s|start_time=%s", arObj);
            outMapCounters.put(strCounterKey, strCounterVal);

        }
    }
}

]]></class_source>
        </definition>
    </definitions>
    <fields>
        <field>
        <field_name>counter_type</field_name>

        <field_type>String</field_type>

        <field_length>-1</field_length>

        <field_precision>-1</field_precision>

        </field>
        <field>
        <field_name>hadoop_job_id</field_name>

        <field_type>String</field_type>

        <field_length>-1</field_length>

        <field_precision>-1</field_precision>

        </field>
        <field>
        <field_name>counter_name</field_name>

        <field_type>String</field_type>

        <field_length>-1</field_length>

        <field_precision>-1</field_precision>

        </field>
        <field>
        <field_name>start_time</field_name>

        <field_type>String</field_type>

        <field_length>-1</field_length>

        <field_precision>-1</field_precision>

        </field>
        <field>
        <field_name>counter_group</field_name>

        <field_type>String</field_type>

        <field_length>-1</field_length>

        <field_precision>-1</field_precision>

        </field>
        <field>
        <field_name>counter_desc</field_name>

        <field_type>String</field_type>

        <field_length>-1</field_length>

        <field_precision>-1</field_precision>

        </field>
        <field>
        <field_name>counter_val</field_name>

        <field_type>String</field_type>

        <field_length>-1</field_length>

        <field_precision>-1</field_precision>

        </field>
        <field>
        <field_name>hadoop_task_id</field_name>

        <field_type>String</field_type>

        <field_length>-1</field_length>

        <field_precision>-1</field_precision>

        </field>
        <field>
        <field_name>general_info</field_name>

        <field_type>String</field_type>

        <field_length>-1</field_length>

        <field_precision>-1</field_precision>

        </field>
    </fields><clear_result_fields>N</clear_result_fields>
<info_steps></info_steps><target_steps></target_steps><usage_parameters></usage_parameters>     <cluster_schema/>
 <remotesteps>   <input>   </input>   <output>   </output> </remotesteps>    <GUI>
      <xloc>317</xloc>
      <yloc>191</yloc>
      <draw>Y</draw>
      </GUI>
    </step>

  <step>
    <name>Write Metrics to LogFile on HDFS</name>
    <type>HadoopFileOutputPlugin</type>
    <description/>
    <distribute>Y</distribute>
    <custom_distribution/>
    <copies>1</copies>
         <partitioning>
           <method>none</method>
           <schema_name/>
           </partitioning>
    <separator>&#x3b;</separator>
    <enclosure>&#x22;</enclosure>
    <enclosure_forced>N</enclosure_forced>
    <enclosure_fix_disabled>N</enclosure_fix_disabled>
    <header>Y</header>
    <footer>N</footer>
    <format>Unix</format>
    <compression>None</compression>
    <encoding>UTF-8</encoding>
    <endedLine/>
    <fileNameInField>Y</fileNameInField>
    <fileNameField>hdfsPath</fileNameField>
    <create_parent_folder>Y</create_parent_folder>
    <file>
      <name>hdfs&#x3a;&#x2f;&#x2f;localhost&#x3a;8020&#x2f;pdi_hadoop_metrics&#x2f;output&#x2f;bm&#x2f;test_m.log</name>
      <is_command>N</is_command>
      <servlet_output>N</servlet_output>
      <do_not_open_new_file_init>Y</do_not_open_new_file_init>
      <extention>log</extention>
      <append>N</append>
      <split>N</split>
      <haspartno>N</haspartno>
      <add_date>N</add_date>
      <add_time>N</add_time>
      <SpecifyFormat>N</SpecifyFormat>
      <date_time_format/>
      <add_to_result_filenames>N</add_to_result_filenames>
      <pad>N</pad>
      <fast_dump>N</fast_dump>
      <splitevery>0</splitevery>
    </file>
    <fields>
      <field>
        <name>dummy</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>counter_type</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>hadoop_job_id</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>counter_name</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>start_time</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>counter_group</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>counter_desc</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>counter_val</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>hadoop_task_id</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>general_info</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>hdfsPath</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
      <field>
        <name>hadoopLogPath</name>
        <type>String</type>
        <format/>
        <currency/>
        <decimal/>
        <group/>
        <nullif/>
        <trim_type>none</trim_type>
        <length>-1</length>
        <precision>-1</precision>
      </field>
    </fields>
     <cluster_schema/>
 <remotesteps>   <input>   </input>   <output>   </output> </remotesteps>    <GUI>
      <xloc>755</xloc>
      <yloc>107</yloc>
      <draw>Y</draw>
      </GUI>
    </step>

  <step>
    <name>Write to log</name>
    <type>WriteToLog</type>
    <description/>
    <distribute>Y</distribute>
    <custom_distribution/>
    <copies>1</copies>
         <partitioning>
           <method>none</method>
           <schema_name/>
           </partitioning>
      <loglevel>log_level_basic</loglevel>
      <displayHeader>Y</displayHeader>
      <limitRows>N</limitRows>
      <limitRowsNumber>0</limitRowsNumber>
      <logmessage/>
    <fields>
      <field>
        <name>counter_type</name>
        </field>
      <field>
        <name>hadoop_job_id</name>
        </field>
      <field>
        <name>counter_name</name>
        </field>
      <field>
        <name>counter_group</name>
        </field>
      <field>
        <name>counter_desc</name>
        </field>
      <field>
        <name>counter_val</name>
        </field>
      </fields>
     <cluster_schema/>
 <remotesteps>   <input>   </input>   <output>   </output> </remotesteps>    <GUI>
      <xloc>556</xloc>
      <yloc>191</yloc>
      <draw>Y</draw>
      </GUI>
    </step>

  <step_error_handling>
  </step_error_handling>
   <slave-step-copy-partition-distribution>
</slave-step-copy-partition-distribution>
   <slave_transformation>N</slave_transformation>
<attributes><group><name>DataService</name>
<attribute><key>name</key>
<value/>
</attribute><attribute><key>stepname</key>
<value/>
</attribute></group></attributes>

</transformation>
